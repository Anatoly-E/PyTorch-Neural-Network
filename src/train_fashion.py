import torch
import torch.optim as optim
from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau
import matplotlib.pyplot as plt
import os
import json
import argparse
from .models import create_model, AdvancedNet
from .utils import load_data, calculate_accuracy, EarlyStopping, visualize_dataset
from .hyperparameter_tuning import HyperparameterTuner
from .config import config

def parse_arguments():
    parser = argparse.ArgumentParser(description='FashionMNIST Experiments')
    parser.add_argument('--experiments', type=str, nargs='+', 
                       choices=['1', '2', '3', '4', '5', 'all'],
                       default=['all'],
                       help='Which experiments to run (1-5 or all)')
    return parser.parse_args()

class FashionMNISTTrainer:
    def __init__(self):
        self.device = config.DEVICE
        os.makedirs('./outputs/models', exist_ok=True)
        os.makedirs('./outputs/plots', exist_ok=True)
        os.makedirs('./outputs/results', exist_ok=True)
    
    def experiment_1_dataset_comparison(self):
        """–≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç 1: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ MNIST –∏ FashionMNIST"""
        print("=" * 60)
        print("–≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢ 1: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤")
        print("=" * 60)
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –æ–±–æ–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
        fashion_train, fashion_test, fashion_dataset = load_data('fashion_mnist', 64)
        mnist_train, mnist_test, mnist_dataset = load_data('mnist', 64)
        
        # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
        visualize_dataset(fashion_dataset, 'FashionMNIST')
        visualize_dataset(mnist_dataset, 'MNIST')
        
        print("\nüìä –û–±—É—á–µ–Ω–∏–µ –Ω–∞ FashionMNIST...")
        # –ú–æ–¥–µ–ª—å –¥–ª—è FashionMNIST
        model_fashion = create_model('medium').to(self.device)
        optimizer_fashion = optim.Adam(model_fashion.parameters(), lr=0.001)
        criterion = torch.nn.CrossEntropyLoss()
        
        fashion_acc = self._quick_train(model_fashion, fashion_train, fashion_test, 
                                    optimizer_fashion, criterion)
        
        print("\nüìä –û–±—É—á–µ–Ω–∏–µ –Ω–∞ MNIST...")
        # –û–¢–î–ï–õ–¨–ù–ê–Ø –º–æ–¥–µ–ª—å –¥–ª—è MNIST
        model_mnist = create_model('medium').to(self.device)
        optimizer_mnist = optim.Adam(model_mnist.parameters(), lr=0.001)
        
        mnist_acc = self._quick_train(model_mnist, mnist_train, mnist_test, 
                                    optimizer_mnist, criterion)
        
        print(f"\nüìà –†–ï–ó–£–õ–¨–¢–ê–¢–´ –°–†–ê–í–ù–ï–ù–ò–Ø:")
        print(f"FashionMNIST —Ç–æ—á–Ω–æ—Å—Ç—å: {fashion_acc:.2f}%")
        print(f"MNIST —Ç–æ—á–Ω–æ—Å—Ç—å: {mnist_acc:.2f}%")
        print(f"–†–∞–∑–Ω–∏—Ü–∞: {abs(fashion_acc - mnist_acc):.2f}%")
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ: —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–µ—Ä–µ–∫—Ä–µ—Å—Ç–Ω–æ (–¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏)
        print(f"\nüîç –ü–µ—Ä–µ–∫—Ä–µ—Å—Ç–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ (–¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –ø—Ä–æ–±–ª–µ–º—ã):")
        print(f"–ú–æ–¥–µ–ª—å FashionMNIST –Ω–∞ –¥–∞–Ω–Ω—ã—Ö MNIST: "
            f"{calculate_accuracy(model_fashion, mnist_test, self.device):.2f}%")
        print(f"–ú–æ–¥–µ–ª—å MNIST –Ω–∞ –¥–∞–Ω–Ω—ã—Ö FashionMNIST: "
            f"{calculate_accuracy(model_mnist, fashion_test, self.device):.2f}%")
        
    def experiment_2_architectures(self):
        """–≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç 2: –†–∞–∑–Ω—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã"""
        print("\n" + "=" * 60)
        print("–≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢ 2: –†–∞–∑–Ω—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –º–æ–¥–µ–ª–µ–π")
        print("=" * 60)
        
        train_loader, test_loader, _ = load_data('fashion_mnist', 64)
        
        architectures = {
            # 1. –†–∞–∑–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä
            'small': create_model('small'),
            'medium': create_model('medium'), 
            'large': create_model('large'),
            
            # 2. –†–∞–∑–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ (–Ω–∞ –æ–¥–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ)
            'relu': create_model('medium', 'relu'),
            'gelu': create_model('medium', 'gelu'),
            'tanh': create_model('medium', 'tanh'),        }
        
        results = {}
        for name, model in architectures.items():
            print(f"\nüèóÔ∏è –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã: {name}")
            model = model.to(self.device)
            optimizer = optim.Adam(model.parameters(), lr=0.001)
            criterion = torch.nn.CrossEntropyLoss()
            
            accuracy = self._quick_train(model, train_loader, test_loader, optimizer, criterion)
            results[name] = accuracy
            print(f"–¢–æ—á–Ω–æ—Å—Ç—å {name}: {accuracy:.2f}%")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        with open('./outputs/results/architecture_results.json', 'w') as f:
            json.dump(results, f, indent=2)
        
        # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
        self._plot_architecture_results(results)

    def experiment_3_hyperparameter_tuning(self):
        """–≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç 3: –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"""
        print("\n" + "=" * 60)
        print("–≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢ 3: Grid Search –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")
        print("=" * 60)
        
        tuner = HyperparameterTuner()
        best_params, all_results = tuner.grid_search('fashion_mnist')
        
        print(f"\nüéØ –õ–£–ß–®–ò–ï –ü–ê–†–ê–ú–ï–¢–†–´:")
        for key, value in best_params.items():
            print(f"  {key}: {value}")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        with open('./outputs/results/grid_search_results.json', 'w') as f:
            json.dump({
                'best_params': best_params,
                'all_results': all_results
            }, f, indent=2)
    
    def experiment_4_early_stopping(self):
        """–≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç 4: –†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞"""
        print("\n" + "=" * 60)
        print("–≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢ 4: Early Stopping")
        print("=" * 60)
        
        train_loader, test_loader, _ = load_data('fashion_mnist', 64)
        model = create_model('medium').to(self.device)
        optimizer = optim.Adam(model.parameters(), lr=0.1)
        criterion = torch.nn.CrossEntropyLoss()
        
        early_stopping = EarlyStopping(patience=2, min_delta=0.01)
        train_losses, val_accuracies = [], []
        
        for epoch in range(20):  # –ú–∞–∫—Å–∏–º—É–º 20 —ç–ø–æ—Ö
            # –û–±—É—á–µ–Ω–∏–µ
            model.train()
            epoch_loss = 0
            for data, target in train_loader:
                data, target = data.to(self.device), target.to(self.device)
                optimizer.zero_grad()
                output = model(data)
                loss = criterion(output, target)
                loss.backward()
                optimizer.step()
                epoch_loss += loss.item()
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è
            accuracy = calculate_accuracy(model, test_loader, self.device)
            train_losses.append(epoch_loss / len(train_loader))
            val_accuracies.append(accuracy)
            
            print(f'Epoch {epoch+1}: Loss: {train_losses[-1]:.4f}, Acc: {accuracy:.2f}%')
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–Ω–Ω–µ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
            if early_stopping(train_losses[-1], model):
                print(f'üõë Early stopping –Ω–∞ —ç–ø–æ—Ö–µ {epoch+1}!')
                break
        
        self._plot_training_curves(train_losses, val_accuracies)
        
    def experiment_5_lr_scheduling(self):
        """–≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç 5: –ü–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ Learning Rate —Å TensorBoard"""
        print("\n" + "=" * 60)
        print("–≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢ 5: LR Scheduling with TensorBoard")
        print("=" * 60)
        
        # üî• –î–û–ë–ê–í–õ–ï–ù–û: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è TensorBoard
        from torch.utils.tensorboard import SummaryWriter
        import os
        from datetime import datetime
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        log_dir = f"./outputs/tensorboard/lr_scheduling_{timestamp}"
        os.makedirs(log_dir, exist_ok=True)
        writer = SummaryWriter(log_dir)
        print(f"üìä TensorBoard: {log_dir}")
        
        # –í–∞—à —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∫–æ–¥ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π...
        train_loader, test_loader, _ = load_data('fashion_mnist', 64)
        
        schedulers_config = [
            {'name': 'StepLR', 'scheduler': None, 'kwargs': {'step_size': 3, 'gamma': 0.1}},
            {'name': 'ReduceLROnPlateau', 'scheduler': None, 'kwargs': {'mode': 'min', 'patience': 2, 'factor': 0.5}}
        ]
        
        for config in schedulers_config:
            print(f"\nüìâ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ {config['name']}...")
            
            model = create_model('medium').to(self.device)
            optimizer = optim.Adam(model.parameters(), lr=0.01)
            criterion = torch.nn.CrossEntropyLoss()
            
            if config['name'] == 'StepLR':
                scheduler = StepLR(optimizer, **config['kwargs'])
            else:
                scheduler = ReduceLROnPlateau(optimizer, **config['kwargs'])
            
            learning_rates = []
            train_losses = []
            accuracies = []
            
            for epoch in range(8):
                model.train()
                epoch_loss = 0
                for data, target in train_loader:
                    data, target = data.to(self.device), target.to(self.device)
                    optimizer.zero_grad()
                    output = model(data)
                    loss = criterion(output, target)
                    loss.backward()
                    optimizer.step()
                    epoch_loss += loss.item()
                
                avg_epoch_loss = epoch_loss / len(train_loader)
                train_losses.append(avg_epoch_loss)
                
                accuracy = calculate_accuracy(model, test_loader, self.device)
                accuracies.append(accuracy)
                
                if config['name'] == 'StepLR':
                    scheduler.step()
                else:
                    scheduler.step(avg_epoch_loss)
                
                current_lr = optimizer.param_groups[0]['lr']
                learning_rates.append(current_lr)
                
                # üî• –î–û–ë–ê–í–õ–ï–ù–û: –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ TensorBoard
                writer.add_scalar(f'LR/{config["name"]}', current_lr, epoch)
                writer.add_scalar(f'Loss/{config["name"]}', avg_epoch_loss, epoch)
                writer.add_scalar(f'Accuracy/{config["name"]}', accuracy, epoch)
                
                print(f'Epoch {epoch+1}: LR={current_lr:.6f}, Loss: {avg_epoch_loss:.4f}, Acc: {accuracy:.2f}%')
            
            self._plot_lr_schedule(learning_rates, accuracies, config['name'])
            self._plot_lr_training_curves(learning_rates, train_losses, accuracies, config['name'])
        
        # üî• –î–û–ë–ê–í–õ–ï–ù–û: –ó–∞–∫—Ä—ã—Ç–∏–µ writer
        writer.close()
        print(f"‚úÖ TensorBoard –ª–æ–≥–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã. –ó–∞–ø—É—Å—Ç–∏—Ç–µ: tensorboard --logdir=./outputs/tensorboard")

    def _plot_lr_training_curves(self, learning_rates, losses, accuracies, scheduler_name):
        """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∫—Ä–∏–≤—ã—Ö –æ–±—É—á–µ–Ω–∏—è —Å LR"""
        fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 4))
        
        # Learning Rate
        ax1.semilogy(learning_rates, 'b-o', linewidth=2, markersize=6)
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Learning Rate')
        ax1.set_title(f'LR Schedule: {scheduler_name}')
        ax1.grid(True)
        
        # Loss
        ax2.plot(losses, 'r-o', linewidth=2, markersize=6)
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('Training Loss')
        ax2.set_title('Training Loss')
        ax2.grid(True)
        
        # Accuracy
        ax3.plot(accuracies, 'g-o', linewidth=2, markersize=6)
        ax3.set_xlabel('Epoch')
        ax3.set_ylabel('Accuracy (%)')
        ax3.set_title('Validation Accuracy')
        ax3.grid(True)
        
        plt.tight_layout()
        plt.savefig(f'./outputs/plots/lr_training_curves_{scheduler_name}.png')
        plt.show()  

    def _quick_train(self, model, train_loader, test_loader, optimizer, criterion, epochs=3):
        """–ë—ã—Å—Ç—Ä–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤"""
        model.train()
        for epoch in range(epochs):
            for data, target in train_loader:
                data, target = data.to(self.device), target.to(self.device)
                optimizer.zero_grad()
                output = model(data)
                loss = criterion(output, target)
                loss.backward()
                optimizer.step()
        
        return calculate_accuracy(model, test_loader, self.device)
    
    def _plot_architecture_results(self, results):
        """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä"""
        plt.figure(figsize=(10, 6))
        names = list(results.keys())
        accuracies = list(results.values())
        
        bars = plt.bar(names, accuracies, color=['skyblue', 'lightcoral', 'lightgreen', 'gold', 'violet'])
        plt.ylabel('–¢–æ—á–Ω–æ—Å—Ç—å (%)')
        plt.title('–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä –º–æ–¥–µ–ª–µ–π –Ω–∞ FashionMNIST')
        plt.ylim(0, 100)
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã
        for bar, acc in zip(bars, accuracies):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, 
                    f'{acc:.1f}%', ha='center', va='bottom')
        
        plt.tight_layout()
        plt.savefig('./outputs/plots/architecture_comparison.png')
        plt.show()
    
    def _plot_training_curves(self, losses, accuracies):
        """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∫—Ä–∏–≤—ã—Ö –æ–±—É—á–µ–Ω–∏—è"""
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))
        
        ax1.plot(losses, 'b-', linewidth=2)
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Loss')
        ax1.set_title('Training Loss')
        ax1.grid(True)
        
        ax2.plot(accuracies, 'r-', linewidth=2)
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('Accuracy (%)')
        ax2.set_title('Validation Accuracy')
        ax2.grid(True)
        
        plt.tight_layout()
        plt.savefig('./outputs/plots/training_curves.png')
        plt.show()
    
    def _plot_lr_schedule(self, learning_rates, accuracies, scheduler_name):
        """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏—è LR"""
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))
        
        ax1.semilogy(learning_rates, 'b-o', linewidth=2, markersize=6)
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Learning Rate')
        ax1.set_title(f'LR Schedule: {scheduler_name}')
        ax1.grid(True)
        
        ax2.plot(accuracies, 'r-o', linewidth=2, markersize=6)
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('Accuracy (%)')
        ax2.set_title('Model Accuracy')
        ax2.grid(True)
        
        plt.tight_layout()
        plt.savefig(f'./outputs/plots/lr_schedule_{scheduler_name}.png')
        plt.show()

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –≤—Å–µ—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤"""
    args = parse_arguments()
    trainer = FashionMNISTTrainer()
    
    experiments_to_run = {
        '1': trainer.experiment_1_dataset_comparison,
        '2': trainer.experiment_2_architectures,
        '3': trainer.experiment_3_hyperparameter_tuning, 
        '4': trainer.experiment_4_early_stopping,
        '5': trainer.experiment_5_lr_scheduling
    }
    
    if 'all' in args.experiments:
        # –ó–∞–ø—É—Å–∫ –≤—Å–µ—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤
        for exp_num, exp_func in experiments_to_run.items():
            print(f"\nüöÄ –ó–∞–ø—É—Å–∫ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ {exp_num}...")
            exp_func()
    else:
        # –ó–∞–ø—É—Å–∫ —Ç–æ–ª—å–∫–æ –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤
        for exp_num in args.experiments:
            if exp_num in experiments_to_run:
                print(f"\nüöÄ –ó–∞–ø—É—Å–∫ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ {exp_num}...")
                experiments_to_run[exp_num]()
    
    print("\nüéâ –í—ã–±—Ä–∞–Ω–Ω—ã–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –∑–∞–≤–µ—Ä—à–µ–Ω—ã!")

if __name__ == "__main__":
    main()