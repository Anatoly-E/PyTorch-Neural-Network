import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import time

# –®–∞–≥ 1: –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ MNIST
def load_mnist(batch_size):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ MNIST"""
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.1307,), (0.3081,))
    ])
    
    train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)
    test_dataset = datasets.MNIST('./data', train=False, download=True, transform=transform)
    
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)
    
    return train_loader, test_loader

# –®–∞–≥ 2: –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω–æ–π —Å–µ—Ç–∏
class ThreeLayerNet(nn.Module):
    def __init__(self, input_size=784, hidden1=128, hidden2=64, output_size=10):
        super(ThreeLayerNet, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden1)
        self.fc2 = nn.Linear(hidden1, hidden2)
        self.fc3 = nn.Linear(hidden2, output_size)
        self.relu = nn.ReLU()
        
    def forward(self, x):
        x = x.view(x.size(0), -1)  # flatten –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ 28x28 –≤ –≤–µ–∫—Ç–æ—Ä 784
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        x = self.fc3(x)
        return x

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏
def calculate_accuracy(model, data_loader, device):
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for data, target in data_loader:
            data, target = data.to(device), target.to(device)
            outputs = model(data)
            _, predicted = torch.max(outputs.data, 1)
            total += target.size(0)
            correct += (predicted == target).sum().item()
    return 100 * correct / total

# –§—É–Ω–∫—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏
def train_model(learning_rate, batch_size, num_epochs=10):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
    print(f"–ü–∞—Ä–∞–º–µ—Ç—Ä—ã: LR={learning_rate}, Batch Size={batch_size}")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    train_loader, test_loader = load_mnist(batch_size)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
    model = ThreeLayerNet().to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    
    # –î–ª—è –∑–∞–ø–∏—Å–∏ –º–µ—Ç—Ä–∏–∫
    train_losses = []
    train_accuracies = []
    test_accuracies = []
    
    start_time = time.time()
    
    for epoch in range(num_epochs):
        model.train()
        running_loss = 0.0
        correct = 0
        total = 0
        
        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(device), target.to(device)
            
            # Forward pass
            outputs = model(data)
            loss = criterion(outputs, target)
            
            # Backward pass –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            running_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += target.size(0)
            correct += (predicted == target).sum().item()
        
        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –¥–ª—è —ç–ø–æ—Ö–∏
        epoch_loss = running_loss / len(train_loader)
        epoch_train_acc = 100 * correct / total
        epoch_test_acc = calculate_accuracy(model, test_loader, device)
        
        train_losses.append(epoch_loss)
        train_accuracies.append(epoch_train_acc)
        test_accuracies.append(epoch_test_acc)
        
        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, '
              f'Train Acc: {epoch_train_acc:.2f}%, Test Acc: {epoch_test_acc:.2f}%')
    
    training_time = time.time() - start_time
    print(f"–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {training_time:.2f} —Å–µ–∫—É–Ω–¥")
    print(f"–§–∏–Ω–∞–ª—å–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —Ç–µ—Å—Ç–µ: {test_accuracies[-1]:.2f}%")
    
    return train_losses, train_accuracies, test_accuracies, model

# –û—Å–Ω–æ–≤–Ω–æ–π —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç
def run_experiments():
    # –®–∞–≥ 3: –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    learning_rates = [0.001, 0.01, 0.1]
    batch_sizes = [32, 64, 128]
    num_epochs = 5
    
    results = {}
    
    for lr in learning_rates:
        for batch_size in batch_sizes:
            print(f"\n{'='*50}")
            print(f"–≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢: LR={lr}, Batch Size={batch_size}")
            print(f"{'='*50}")
            
            key = f"LR_{lr}_BS_{batch_size}"
            train_losses, train_acc, test_acc, model = train_model(
                learning_rate=lr, 
                batch_size=batch_size, 
                num_epochs=num_epochs
            )
            
            results[key] = {
                'train_losses': train_losses,
                'train_accuracies': train_acc,
                'test_accuracies': test_acc,
                'final_test_accuracy': test_acc[-1],
                'model': model
            }
    
    return results

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
def plot_results(results):
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
    
    # –ì—Ä–∞—Ñ–∏–∫ –ø–æ—Ç–µ—Ä—å –¥–ª—è —Ä–∞–∑–Ω—ã—Ö learning rates (–ø—Ä–∏ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–º batch size=64)
    ax1.set_title('Loss –¥–ª—è —Ä–∞–∑–Ω—ã—Ö Learning Rates (BS=64)')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Loss')
    for lr in [0.001, 0.01, 0.1]:
        key = f"LR_{lr}_BS_64"
        if key in results:
            ax1.plot(results[key]['train_losses'], label=f'LR={lr}')
    ax1.legend()
    ax1.grid(True)
    
    # –ì—Ä–∞—Ñ–∏–∫ —Ç–æ—á–Ω–æ—Å—Ç–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö learning rates
    ax2.set_title('Test Accuracy –¥–ª—è —Ä–∞–∑–Ω—ã—Ö Learning Rates (BS=64)')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('Accuracy (%)')
    for lr in [0.001, 0.01, 0.1]:
        key = f"LR_{lr}_BS_64"
        if key in results:
            ax2.plot(results[key]['test_accuracies'], label=f'LR={lr}')
    ax2.legend()
    ax2.grid(True)
    
    # –ì—Ä–∞—Ñ–∏–∫ —Ç–æ—á–Ω–æ—Å—Ç–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö batch sizes (–ø—Ä–∏ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–º lr=0.001)
    ax3.set_title('Test Accuracy –¥–ª—è —Ä–∞–∑–Ω—ã—Ö Batch Sizes (LR=0.001)')
    ax3.set_xlabel('Epoch')
    ax3.set_ylabel('Accuracy (%)')
    for bs in [32, 64, 128]:
        key = f"LR_0.001_BS_{bs}"
        if key in results:
            ax3.plot(results[key]['test_accuracies'], label=f'BS={bs}')
    ax3.legend()
    ax3.grid(True)
    
    # –°–≤–æ–¥–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    final_accuracies = []
    configs = []
    for key, result in results.items():
        final_accuracies.append(result['final_test_accuracy'])
        configs.append(key)
    
    ax4.axis('off')
    table_data = []
    for config, acc in zip(configs, final_accuracies):
        table_data.append([config, f"{acc:.2f}%"])
    
    table = ax4.table(cellText=table_data, 
                     colLabels=['Configuration', 'Final Test Accuracy'], 
                     loc='center',
                     cellLoc='center')
    table.auto_set_font_size(False)
    table.set_fontsize(10)
    table.scale(1, 1.5)
    
    plt.tight_layout()
    plt.show()

# –ó–∞–ø—É—Å–∫ –≤—Å–µ–π –ª–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω–æ–π —Ä–∞–±–æ—Ç—ã
if __name__ == "__main__":
    print("–ù–∞—á–∞–ª–æ –ª–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω–æ–π —Ä–∞–±–æ—Ç—ã: –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è MNIST")
    results = run_experiments()
    plot_results(results)
    
    # –í—ã–≤–æ–¥ –ª—É—á—à–µ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
    best_config = max(results.items(), key=lambda x: x[1]['final_test_accuracy'])
    print(f"\nüéØ –õ–£–ß–®–ò–ô –†–ï–ó–£–õ–¨–¢–ê–¢:")
    print(f"–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {best_config[0]}")
    print(f"–¢–æ—á–Ω–æ—Å—Ç—å: {best_config[1]['final_test_accuracy']:.2f}%")