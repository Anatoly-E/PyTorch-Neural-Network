Дополнительная часть:

1. Смена датасета: Заменить датасет MNIST на FashionMNIST. Обучить модель той же
архитектуры. Проанализировать и объяснить разницу в результатах и сложности задачи.
2. Эксперименты с архитектурой:
o Изменить количество и размер линейных слоев.
o Добавить слои Dropout и Batch Normalization.
o Опробовать разные функции активации (ReLU, GELU, Tanh).
3. Поиск гиперпараметров: Реализовать простой Grid Search по следующим параметрам:
o Learning rate (например, [0.1, 0.01, 0.001]).
o Batch size (например, [32, 64, 128]).
o Оптимизатор ([SGD, Adam]).
4. Ранняя остановка (Early Stopping): Реализовать callback Early Stopping, который
прекращает обучение, если метрика на проверочной выборке не улучшается в течение
заданного количества эпох (patience).
5. Планировщик скорости обучения (LR Scheduler):
o Интегрировать любой планировщик (например, StepLR или ReduceLROnPlateau).
o Визуализировать изменение learning rate в процессе обучения с помощью
TensorBoard.

Можно запустить один из экспериментов, например, первый
Python main.py --experiments 1 2 3 4 5 

Можно запустить несколько экспериментов, например, первый и третий
Python main.py --experiments 1 3 

Можно запустить все 
Python main.py --experiments all
Python main.py
